{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36777c0d",
   "metadata": {},
   "source": [
    "# Homework: Speed Up Your dlt Pipeline\n",
    "\n",
    "## Goal\n",
    "Build a `dlt` pipeline using the **Jaffle Shop API** and apply performance optimization techniques:\n",
    "- Chunking (yielding pages instead of rows)\n",
    "- Parallelism (`parallelized=True`)\n",
    "- Buffer control\n",
    "- File rotation\n",
    "- Worker tuning\n",
    "\n",
    "## API Information\n",
    "- **Base URL**: `https://jaffle-shop.scalevector.ai/api/v1`\n",
    "- **Docs**: https://jaffle-shop.scalevector.ai/docs\n",
    "- **Endpoints**: `/customers`, `/orders`, `/products`\n",
    "- **Pagination**: Uses `page` and `page_size` query parameters with `Link` header for next page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bb669",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -U \"dlt[duckdb]\" requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db29f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import dlt\n",
    "import requests\n",
    "from typing import Iterator, Dict, Any, List\n",
    "from threading import current_thread\n",
    "from dlt.common.typing import TDataItems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041de7ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Naive Implementation (Baseline)\n",
    "\n",
    "This version uses:\n",
    "- No parallelization\n",
    "- Yielding one row at a time\n",
    "- Default worker settings\n",
    "- No file rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ec02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment variables to defaults\n",
    "for key in [\"EXTRACT__WORKERS\", \"NORMALIZE__WORKERS\", \"LOAD__WORKERS\", \n",
    "            \"DATA_WRITER__BUFFER_MAX_ITEMS\", \"EXTRACT__DATA_WRITER__FILE_MAX_ITEMS\",\n",
    "            \"NORMALIZE__DATA_WRITER__FILE_MAX_ITEMS\"]:\n",
    "    os.environ.pop(key, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e31e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE = \"https://jaffle-shop.scalevector.ai/api/v1\"\n",
    "\n",
    "\n",
    "def naive_paginate(endpoint: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Naive pagination - yields one row at a time (inefficient).\"\"\"\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{API_BASE}/{endpoint}?page={page}&page_size=100\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        # Yielding one row at a time - INEFFICIENT!\n",
    "        for item in data:\n",
    "            yield item\n",
    "        \n",
    "        # Check for next page using Link header\n",
    "        if \"next\" not in response.links:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "\n",
    "@dlt.resource(table_name=\"customers\", write_disposition=\"replace\", primary_key=\"id\")\n",
    "def naive_customers() -> TDataItems:\n",
    "    \"\"\"Naive customers resource - no parallelization, yields rows.\"\"\"\n",
    "    yield from naive_paginate(\"customers\")\n",
    "\n",
    "\n",
    "@dlt.resource(table_name=\"orders\", write_disposition=\"replace\", primary_key=\"id\")\n",
    "def naive_orders() -> TDataItems:\n",
    "    \"\"\"Naive orders resource - no parallelization, yields rows.\"\"\"\n",
    "    yield from naive_paginate(\"orders\")\n",
    "\n",
    "\n",
    "@dlt.resource(table_name=\"products\", write_disposition=\"replace\", primary_key=\"sku\")\n",
    "def naive_products() -> TDataItems:\n",
    "    \"\"\"Naive products resource - no parallelization, yields rows.\"\"\"\n",
    "    yield from naive_paginate(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb36ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NAIVE PIPELINE (Baseline)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "NAIVE TOTAL TIME: 394.52 seconds\n",
      "============================================================\n",
      "Run started at 2026-01-15 11:04:59.984777+00:00 and COMPLETED in 6 minutes and 34.50 seconds with 4 steps.\n",
      "Step extract COMPLETED in 6 minutes and 23.53 seconds.\n",
      "\n",
      "Load package 1768475099.997995 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 3.83 seconds.\n",
      "Normalized data for the following tables:\n",
      "- products: 10 row(s)\n",
      "- orders: 61948 row(s)\n",
      "- orders__items: 90900 row(s)\n",
      "- customers: 935 row(s)\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "\n",
      "Load package 1768475099.997995 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 7.14 seconds.\n",
      "Pipeline jaffle_shop_naive load step completed in 6.21 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_naive_20260115110459\n",
      "The duckdb destination used duckdb:////Users/bartoszturkowyd/Projects/Python/dlt-fundamentals-cert/jaffle_shop_naive.duckdb location to store data\n",
      "Load package 1768475099.997995 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 6 minutes and 34.50 seconds.\n",
      "Pipeline jaffle_shop_naive load step completed in 6.21 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_naive_20260115110459\n",
      "The duckdb destination used duckdb:////Users/bartoszturkowyd/Projects/Python/dlt-fundamentals-cert/jaffle_shop_naive.duckdb location to store data\n",
      "Load package 1768475099.997995 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Run the naive pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"NAIVE PIPELINE (Baseline)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "naive_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"jaffle_shop_naive\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"jaffle_naive\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "naive_start = time.perf_counter()\n",
    "\n",
    "# Run the full pipeline\n",
    "load_info = naive_pipeline.run([naive_customers, naive_orders, naive_products])\n",
    "\n",
    "naive_total_time = time.perf_counter() - naive_start\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"NAIVE TOTAL TIME: {naive_total_time:.2f} seconds\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(naive_pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b570b8e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Optimized Implementation\n",
    "\n",
    "This version applies all performance techniques:\n",
    "1. **Chunking**: Yield pages instead of individual rows\n",
    "2. **Parallelism**: Use `parallelized=True` on resources\n",
    "3. **Source grouping**: Group resources into a single source\n",
    "4. **Worker tuning**: Configure extract, normalize, and load workers\n",
    "5. **Buffer control**: Optimize buffer sizes\n",
    "6. **File rotation**: Enable file rotation for better parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dd7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured workers:\n",
      "  - Extract workers: 3\n",
      "  - Normalize workers: 8\n",
      "  - Load workers: 5\n",
      "  - Buffer max items: 10000\n",
      "  - Extract file rotation: 5000 items\n",
      "  - Normalize file rotation: 5000 items\n"
     ]
    }
   ],
   "source": [
    "# Configure performance settings\n",
    "os.environ[\"EXTRACT__WORKERS\"] = \"3\"  # 3 resources = 3 extract workers\n",
    "os.environ[\"NORMALIZE__WORKERS\"] = str(os.cpu_count() or 2)  # Use all CPU cores\n",
    "os.environ[\"LOAD__WORKERS\"] = \"5\"  # Multiple load threads\n",
    "os.environ[\"DATA_WRITER__BUFFER_MAX_ITEMS\"] = \"10000\"  # Larger buffer for fewer writes\n",
    "os.environ[\"EXTRACT__DATA_WRITER__FILE_MAX_ITEMS\"] = \"5000\"  # Enable file rotation\n",
    "os.environ[\"NORMALIZE__DATA_WRITER__FILE_MAX_ITEMS\"] = \"5000\"  # Enable file rotation\n",
    "\n",
    "print(\"Configured workers:\")\n",
    "print(f\"  - Extract workers: {os.environ['EXTRACT__WORKERS']}\")\n",
    "print(f\"  - Normalize workers: {os.environ['NORMALIZE__WORKERS']}\")\n",
    "print(f\"  - Load workers: {os.environ['LOAD__WORKERS']}\")\n",
    "print(f\"  - Buffer max items: {os.environ['DATA_WRITER__BUFFER_MAX_ITEMS']}\")\n",
    "print(f\"  - Extract file rotation: {os.environ['EXTRACT__DATA_WRITER__FILE_MAX_ITEMS']} items\")\n",
    "print(f\"  - Normalize file rotation: {os.environ['NORMALIZE__DATA_WRITER__FILE_MAX_ITEMS']} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9d8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_paginate(endpoint: str) -> Iterator[List[Dict[str, Any]]]:\n",
    "    \"\"\"Optimized pagination - yields entire pages (chunks) instead of rows.\"\"\"\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{API_BASE}/{endpoint}?page={page}&page_size=100\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        # Yield the entire page as a chunk - EFFICIENT!\n",
    "        yield data\n",
    "        \n",
    "        # Check for next page using Link header\n",
    "        if \"next\" not in response.links:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"customers\",\n",
    "    write_disposition=\"replace\",\n",
    "    primary_key=\"id\",\n",
    "    parallelized=True  # Enable parallel extraction\n",
    ")\n",
    "def optimized_customers() -> TDataItems:\n",
    "    \"\"\"Optimized customers resource - parallelized, yields pages.\"\"\"\n",
    "    print(f\"  [customers] Running in thread: {current_thread().name}\")\n",
    "    yield from optimized_paginate(\"customers\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"orders\",\n",
    "    write_disposition=\"replace\",\n",
    "    primary_key=\"id\",\n",
    "    parallelized=True  # Enable parallel extraction\n",
    ")\n",
    "def optimized_orders() -> TDataItems:\n",
    "    \"\"\"Optimized orders resource - parallelized, yields pages.\"\"\"\n",
    "    print(f\"  [orders] Running in thread: {current_thread().name}\")\n",
    "    yield from optimized_paginate(\"orders\")\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"products\",\n",
    "    write_disposition=\"replace\",\n",
    "    primary_key=\"sku\",\n",
    "    parallelized=True  # Enable parallel extraction\n",
    ")\n",
    "def optimized_products() -> TDataItems:\n",
    "    \"\"\"Optimized products resource - parallelized, yields pages.\"\"\"\n",
    "    print(f\"  [products] Running in thread: {current_thread().name}\")\n",
    "    yield from optimized_paginate(\"products\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def jaffle_shop_source():\n",
    "    \"\"\"Group all resources into a single source for better scheduling.\"\"\"\n",
    "    return optimized_customers, optimized_orders, optimized_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bbe74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZED PIPELINE\n",
      "============================================================\n",
      "  [customers] Running in thread: dlt-pool-8673609728-extract-threads_0\n",
      "  [orders] Running in thread: dlt-pool-8673609728-extract-threads_1\n",
      "  [products] Running in thread: dlt-pool-8673609728-extract-threads_2\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED TOTAL TIME: 234.01 seconds\n",
      "============================================================\n",
      "Run started at 2026-01-15 11:11:46.857589+00:00 and COMPLETED in 3 minutes and 54.02 seconds with 4 steps.\n",
      "Step extract COMPLETED in 3 minutes and 43.28 seconds.\n",
      "\n",
      "Load package 1768475506.865561 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 4.80 seconds.\n",
      "Normalized data for the following tables:\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- customers: 935 row(s)\n",
      "- products: 10 row(s)\n",
      "- orders: 61948 row(s)\n",
      "- orders__items: 90900 row(s)\n",
      "\n",
      "Load package 1768475506.865561 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 5.93 seconds.\n",
      "Pipeline jaffle_shop_optimized load step completed in 5.92 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_optimized_20260115111146\n",
      "The duckdb destination used duckdb:////Users/bartoszturkowyd/Projects/Python/dlt-fundamentals-cert/jaffle_shop_optimized.duckdb location to store data\n",
      "Load package 1768475506.865561 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 3 minutes and 54.02 seconds.\n",
      "Pipeline jaffle_shop_optimized load step completed in 5.92 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset jaffle_optimized_20260115111146\n",
      "The duckdb destination used duckdb:////Users/bartoszturkowyd/Projects/Python/dlt-fundamentals-cert/jaffle_shop_optimized.duckdb location to store data\n",
      "Load package 1768475506.865561 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Run the optimized pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZED PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimized_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"jaffle_shop_optimized\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"jaffle_optimized\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "optimized_start = time.perf_counter()\n",
    "\n",
    "# Run the full pipeline using the source\n",
    "load_info = optimized_pipeline.run(jaffle_shop_source())\n",
    "\n",
    "optimized_total_time = time.perf_counter() - optimized_start\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"OPTIMIZED TOTAL TIME: {optimized_total_time:.2f} seconds\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(optimized_pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7642e6d",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f657b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON SUMMARY\n",
      "============================================================\n",
      "\n",
      "Naive Pipeline:     394.52 seconds\n",
      "Optimized Pipeline: 234.01 seconds\n",
      "\n",
      "ðŸš€ Speedup: 1.69x faster!\n",
      "   Time saved: 160.51 seconds\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNaive Pipeline:     {naive_total_time:.2f} seconds\")\n",
    "print(f\"Optimized Pipeline: {optimized_total_time:.2f} seconds\")\n",
    "\n",
    "if naive_total_time > optimized_total_time:\n",
    "    speedup = naive_total_time / optimized_total_time\n",
    "    print(f\"\\nSpeedup: {speedup:.2f}x faster\")\n",
    "    print(f\"Time saved: {naive_total_time - optimized_total_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"\\nThe optimized version was not faster.\")\n",
    "    print(\"This can happen with small datasets where parallelization overhead\")\n",
    "    print(\"exceeds the benefits.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8f39",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Data Verification\n",
    "\n",
    "Verify that both pipelines loaded the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc73ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive dataset: jaffle_naive_20260115110459\n",
      "Optimized dataset: jaffle_optimized_20260115111146\n",
      "\n",
      "Row counts comparison:\n",
      "----------------------------------------\n",
      "customers    | Naive:   935 | Optimized:   935 | âœ…\n",
      "orders       | Naive: 61948 | Optimized: 61948 | âœ…\n",
      "products     | Naive:    10 | Optimized:    10 | âœ…\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the DuckDB databases\n",
    "naive_conn = duckdb.connect(f\"{naive_pipeline.pipeline_name}.duckdb\")\n",
    "optimized_conn = duckdb.connect(f\"{optimized_pipeline.pipeline_name}.duckdb\")\n",
    "\n",
    "# Get the actual dataset names from the pipelines\n",
    "naive_dataset = naive_pipeline.dataset_name\n",
    "optimized_dataset = optimized_pipeline.dataset_name\n",
    "\n",
    "print(f\"Naive dataset: {naive_dataset}\")\n",
    "print(f\"Optimized dataset: {optimized_dataset}\")\n",
    "print(\"\\nRow counts comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for table in [\"customers\", \"orders\", \"products\"]:\n",
    "    naive_count = naive_conn.execute(f\"SELECT COUNT(*) FROM {naive_dataset}.{table}\").fetchone()[0]\n",
    "    optimized_count = optimized_conn.execute(f\"SELECT COUNT(*) FROM {optimized_dataset}.{table}\").fetchone()[0]\n",
    "    match = \"MATCH\" if naive_count == optimized_count else \"MISMATCH\"\n",
    "    print(f\"{table:12} | Naive: {naive_count:5} | Optimized: {optimized_count:5} | {match}\")\n",
    "\n",
    "naive_conn.close()\n",
    "optimized_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7c618",
   "metadata": {},
   "source": [
    "---\n",
    "# Explanation: What Changes Made the Biggest Difference?\n",
    "\n",
    "The optimized pipeline achieved a 1.69x speedup (394 seconds to 234 seconds).\n",
    "\n",
    "## Key Changes by Impact\n",
    "\n",
    "**High Impact:**\n",
    "1. Yielding pages instead of rows - reduced yield operations from 62,893 to ~629\n",
    "2. Parallel extraction (`parallelized=True`) - resources run concurrently in separate threads\n",
    "\n",
    "**Medium Impact:**\n",
    "3. Worker configuration - 3 extract workers, 8 normalize workers, 5 load workers\n",
    "4. Source grouping - enables dlt's resource scheduling\n",
    "\n",
    "**Low Impact:**\n",
    "5. Buffer size increase (5,000 to 10,000 items)\n",
    "6. File rotation (limited effect at this data volume)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The extract stage was the primary bottleneck. Chunking and parallelization provided the most significant improvements by reducing function call overhead and enabling concurrent API requests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt-fundamentals-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
